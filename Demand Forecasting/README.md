# üìò AV Demand Forecasting - Inference Guide & Technical Documentation

**Version:** 1.0  
**Last Updated:** November 2025
**Model Type:** LightGBM Regressor with 10-Fold Cross Validation

---

## üìã Table of Contents

1. [Executive Summary](#executive-summary)
2. [Model Overview](#model-overview)
3. [Data Structure & Requirements](#data-structure--requirements)
4. [Feature Engineering Pipeline](#feature-engineering-pipeline)
5. [Inference Process](#inference-process)
6. [Input Specifications](#input-specifications)
7. [Output Specifications](#output-specifications)
8. [Code Examples](#code-examples)
9. [Performance Metrics](#performance-metrics)
10. [Troubleshooting](#troubleshooting)
11. [Appendix](#appendix)

---

## 1. Executive Summary

### 1.1 Purpose
Model n√†y d·ª± ƒëo√°n **units_sold** (s·ªë l∆∞·ª£ng s·∫£n ph·∫©m b√°n ƒë∆∞·ª£c) cho m·ªói combination c·ªßa (store_id, sku_id, week).

### 1.2 Key Metrics
- **Validation RMSLE:** ~327
- **Expected Public LB:** ~360-365
- **Training Time:** ~15-20 ph√∫t (10-fold CV)
- **Inference Time:** <1 gi√¢y cho 13,860 samples

### 1.3 Technology Stack
```
Python 3.7+
‚îú‚îÄ‚îÄ pandas >= 1.3.0
‚îú‚îÄ‚îÄ numpy >= 1.21.0
‚îú‚îÄ‚îÄ lightgbm >= 3.3.0
‚îú‚îÄ‚îÄ category-encoders >= 2.5.0
‚îî‚îÄ‚îÄ scikit-learn >= 1.0.0
```

---

## 2. Model Overview

### 2.1 Model Architecture

```
Input Data (Test Set)
    ‚Üì
Feature Engineering Pipeline
    ‚îú‚îÄ‚îÄ Price Features (3 features)
    ‚îú‚îÄ‚îÄ Categorical Encoding (4 features)
    ‚îî‚îÄ‚îÄ DateTime Features (12 features)
    ‚Üì
Total: 23 Features
    ‚Üì
LightGBM DART Regressor
    ‚îú‚îÄ‚îÄ 10-Fold Cross Validation
    ‚îú‚îÄ‚îÄ 575 iterations per fold
    ‚îî‚îÄ‚îÄ Early stopping (30 rounds)
    ‚Üì
Ensemble Predictions (average of 10 models)
    ‚Üì
Output: units_sold predictions
```

### 2.2 Model Hyperparameters

```python
params = {
    'boosting_type': 'dart',           # Dropout for trees
    'objective': 'regression',         # Regression task
    'metric': 'l1',                    # MAE metric
    'learning_rate': 0.5,              # High learning rate
    'min_data_in_leaf': 15,            # Minimum samples per leaf
    'bagging_fraction': 0.7,           # 70% data sampling
    'feature_fraction': 0.7,           # 70% feature sampling
    'bagging_seed': 50                 # Random seed
}
```

### 2.3 Target Transformation

**Important:** Target variable ƒë∆∞·ª£c transform b·∫±ng **log1p** (log(x+1)) tr∆∞·ªõc khi training.

```python
# Training
y_train_transformed = np.log1p(y_train)

# Prediction (ph·∫£i inverse transform)
y_pred_original = np.exp(y_pred_transformed)
```

**L√Ω do:**
- Target distribution b·ªã skewed (ph√¢n ph·ªëi l·ªách)
- Log transformation l√†m ph√¢n ph·ªëi g·∫ßn Normal h∆°n
- Gi√∫p model h·ªçc t·ªët h∆°n

---

## 3. Data Structure & Requirements

### 3.1 Input Data Format

**File:** CSV format v·ªõi c√°c columns b·∫Øt bu·ªôc

| Column Name | Data Type | Description | Example |
|-------------|-----------|-------------|---------|
| `record_ID` | int64 | Unique identifier cho m·ªói record | 212645 |
| `week` | string | Ng√†y b·∫Øt ƒë·∫ßu tu·∫ßn (format: DD/MM/YY) | "16/07/13" |
| `store_id` | int64 | ID c·ªßa c·ª≠a h√†ng | 8091 |
| `sku_id` | int64 | ID c·ªßa s·∫£n ph·∫©m (SKU) | 216418 |
| `total_price` | float64 | Gi√° b√°n th·ª±c t·∫ø | 108.30 |
| `base_price` | float64 | Gi√° g·ªëc | 108.30 |
| `is_featured_sku` | int64 | S·∫£n ph·∫©m c√≥ ƒë∆∞·ª£c feature kh√¥ng (0/1) | 0 |
| `is_display_sku` | int64 | S·∫£n ph·∫©m c√≥ ƒë∆∞·ª£c display kh√¥ng (0/1) | 0 |

**L∆∞u √Ω:**
- `week` ph·∫£i theo format `DD/MM/YY` (v√≠ d·ª•: 16/07/13)
- `is_featured_sku` v√† `is_display_sku` ch·ªâ nh·∫≠n gi√° tr·ªã 0 ho·∫∑c 1
- Kh√¥ng ƒë∆∞·ª£c c√≥ missing values trong c√°c columns tr·ª´ `total_price`

### 3.2 Sample Input Data

```csv
record_ID,week,store_id,sku_id,total_price,base_price,is_featured_sku,is_display_sku
212645,16/07/13,8091,216418,108.3000,108.3000,0,0
212646,16/07/13,8091,216419,109.0125,109.0125,0,0
212647,16/07/13,8091,216425,133.9500,133.9500,0,0
```

---

## 4. Feature Engineering Pipeline

### 4.1 Overview

T·ª´ 8 features g·ªëc, model t·∫°o ra **23 features** th√¥ng qua feature engineering:

```
Original Features (8)
    ‚îú‚îÄ‚îÄ record_ID (dropped)
    ‚îú‚îÄ‚îÄ week (transformed to 6 datetime features)
    ‚îú‚îÄ‚îÄ store_id (kept + encoded)
    ‚îú‚îÄ‚îÄ sku_id (kept + encoded)
    ‚îú‚îÄ‚îÄ total_price (kept + derived)
    ‚îú‚îÄ‚îÄ base_price (kept + derived)
    ‚îú‚îÄ‚îÄ is_featured_sku (kept)
    ‚îî‚îÄ‚îÄ is_display_sku (kept)
        ‚Üì
Engineered Features (23)
```

### 4.2 Price Features (3 features)

#### 4.2.1 `diff`
**ƒê·ªãnh nghƒ©a:** Ch√™nh l·ªách gi·ªØa base_price v√† total_price

```python
diff = base_price - total_price
```

**√ù nghƒ©a:**
- M·ª©c discount (gi·∫£m gi√°)
- `diff > 0`: C√≥ discount
- `diff = 0`: Kh√¥ng c√≥ discount
- `diff < 0`: TƒÉng gi√° (hi·∫øm)

**Example:**
```
base_price = 111.86, total_price = 99.04
diff = 111.86 - 99.04 = 12.82
‚Üí S·∫£n ph·∫©m ƒë∆∞·ª£c gi·∫£m 12.82 ƒë∆°n v·ªã ti·ªÅn
```

#### 4.2.2 `relative_diff_base`
**ƒê·ªãnh nghƒ©a:** % discount so v·ªõi base_price

```python
relative_diff_base = diff / base_price
```

**√ù nghƒ©a:**
- T·ª∑ l·ªá discount
- Range: [0, 1] th∆∞·ªùng
- Cao = discount nhi·ªÅu

**Example:**
```
diff = 12.82, base_price = 111.86
relative_diff_base = 12.82 / 111.86 = 0.1147 (11.47% discount)
```

#### 4.2.3 `relative_diff_total`
**ƒê·ªãnh nghƒ©a:** Markup ratio so v·ªõi total_price

```python
relative_diff_total = diff / total_price
```

**√ù nghƒ©a:**
- T·ª∑ l·ªá discount t√≠nh theo gi√° b√°n
- Th∆∞·ªùng cao h∆°n `relative_diff_base`

**Example:**
```
diff = 12.82, total_price = 99.04
relative_diff_total = 12.82 / 99.04 = 0.1295 (12.95%)
```

### 4.3 Categorical Encoding (4 features)

#### 4.3.1 M-Estimate Encoding

**Technique:** Target-based encoding using M-Estimate

**Formula:**
```
encoded_value = (n * mean_target + m * global_mean) / (n + m)

where:
- n = s·ªë l∆∞·ª£ng samples c√≥ value ƒë√≥
- mean_target = mean c·ªßa target cho value ƒë√≥
- global_mean = mean to√†n b·ªô dataset
- m = regularization parameter (default=1)
```

**Features:**
1. `store_encoded`: M-estimate encoding c·ªßa store_id
2. `sku_encoded`: M-estimate encoding c·ªßa sku_id
3. `store_id`: Original ID (kept as categorical)
4. `sku_id`: Original ID (kept as categorical)

**Why M-Estimate?**
- Handle high cardinality (76 stores, 28 SKUs)
- Prevent overfitting
- Better than one-hot encoding
- Incorporate target information

**Example:**
```python
# Store 8091 xu·∫•t hi·ªán 130 l·∫ßn trong training v·ªõi mean units_sold = 52.3
# Global mean units_sold = 51.67

store_encoded = (130 * 52.3 + 1 * 51.67) / (130 + 1)
              = 6850.67 / 131
              = 52.29
```

### 4.4 DateTime Features (12 features)

**Base Date:** 17/01/2011 (reference point)

#### 4.4.1 Week Start Features (6 features)

| Feature | Description | Range | Example |
|---------|-------------|-------|---------|
| `year` | NƒÉm c·ªßa week | 2011-2013 | 2013 |
| `month` | Th√°ng c·ªßa week | 1-12 | 7 |
| `date` | Ng√†y trong th√°ng | 1-31 | 16 |
| `weekday` | Th·ª© trong tu·∫ßn (0=Monday) | 0-6 | 1 (Tuesday) |
| `weeknum` | Tu·∫ßn th·ª© m·∫•y trong nƒÉm | 1-53 | 28 |
| `week_serial` | S·ªë tu·∫ßn k·ªÉ t·ª´ base date | 0-140 | 129.14 |

**Calculation Example:**
```python
week = "16/07/13" ‚Üí datetime(2013, 7, 16)
base_date = datetime(2011, 1, 17)

week_serial = (week - base_date).days / 7
            = 911 / 7
            = 129.14 weeks
```

#### 4.4.2 Week End Features (6 features)

**Concept:** Weekend date = Week start + 6 days

| Feature | Description | Range | Example |
|---------|-------------|-------|---------|
| `end_year` | NƒÉm c·ªßa weekend | 2011-2013 | 2013 |
| `end_month` | Th√°ng c·ªßa weekend | 1-12 | 7 |
| `end_date` | Ng√†y c·ªßa weekend | 1-31 | 22 |
| `end_weekday` | Th·ª© c·ªßa weekend (0=Monday) | 0-6 | 0 (Monday) |
| `end_weeknum` | Tu·∫ßn c·ªßa weekend | 1-53 | 29 |
| `end_week_serial` | Serial number | 0-141 | 130.0 |

**Why Week End Features?**
- Capture weekly patterns
- Some stores c√≥ behavior kh√°c ng√†y cu·ªëi tu·∫ßn
- Seasonality detection

### 4.5 Feature Summary Table

| Category | Count | Features |
|----------|-------|----------|
| **Original** | 5 | base_price, total_price, is_featured_sku, is_display_sku, store_id, sku_id |
| **Price Derived** | 3 | diff, relative_diff_base, relative_diff_total |
| **Categorical Encoded** | 2 | store_encoded, sku_encoded |
| **DateTime** | 12 | year, month, date, weekday, weeknum, week_serial, end_year, end_month, end_date, end_weekday, end_weeknum, end_week_serial |
| **TOTAL** | 23 | - |

---

## 5. Inference Process

### 5.1 Complete Pipeline Flowchart

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Load Raw Test Data (CSV)           ‚îÇ
‚îÇ     - 8 columns                         ‚îÇ
‚îÇ     - 13,860 rows                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. Data Validation                     ‚îÇ
‚îÇ     - Check required columns            ‚îÇ
‚îÇ     - Check data types                  ‚îÇ
‚îÇ     - Check value ranges                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. Data Preprocessing                  ‚îÇ
‚îÇ     - Fill missing total_price          ‚îÇ
‚îÇ     - Create store_sku identifier       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. Feature Engineering                 ‚îÇ
‚îÇ     ‚îú‚îÄ Price Features                   ‚îÇ
‚îÇ     ‚îú‚îÄ Categorical Encoding             ‚îÇ
‚îÇ     ‚îî‚îÄ DateTime Features                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. Load Trained Models                 ‚îÇ
‚îÇ     - 10 LightGBM models (from CV)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. Make Predictions                    ‚îÇ
‚îÇ     - Predict v·ªõi m·ªói fold model        ‚îÇ
‚îÇ     - Transform: np.exp(predictions)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  7. Ensemble Predictions                ‚îÇ
‚îÇ     - Average 10 predictions            ‚îÇ
‚îÇ     - Apply np.abs() (ensure positive)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  8. Format Output                       ‚îÇ
‚îÇ     - record_ID, units_sold             ‚îÇ
‚îÇ     - Save to CSV                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 Step-by-Step Inference Guide

#### Step 1: Load Test Data

```python
import pandas as pd

# Load test data
test = pd.read_csv('test_data.csv')

# Verify shape
print(f"Test data shape: {test.shape}")
# Expected: (n_samples, 8)
```

#### Step 2: Data Validation

```python
# Required columns
required_cols = ['record_ID', 'week', 'store_id', 'sku_id', 
                 'total_price', 'base_price', 'is_featured_sku', 'is_display_sku']

# Check columns
assert all(col in test.columns for col in required_cols), "Missing required columns!"

# Check data types
assert test['store_id'].dtype == 'int64', "store_id must be int64"
assert test['sku_id'].dtype == 'int64', "sku_id must be int64"

# Check value ranges
assert test['is_featured_sku'].isin([0, 1]).all(), "is_featured_sku must be 0 or 1"
assert test['is_display_sku'].isin([0, 1]).all(), "is_display_sku must be 0 or 1"

print("‚úì Data validation passed!")
```

#### Step 3: Data Preprocessing

```python
# Fill missing total_price with base_price
test['total_price'] = test['total_price'].fillna(test['base_price'])

# Create store_sku identifier (optional, for tracking)
test['store_sku'] = (test['store_id'].astype('str') + "_" + 
                     test['sku_id'].astype('str'))
```

#### Step 4: Feature Engineering

**A. Price Features**
```python
test['diff'] = test['base_price'] - test['total_price']
test['relative_diff_base'] = test['diff'] / test['base_price']
test['relative_diff_total'] = test['diff'] / test['total_price']
```

**B. Categorical Encoding**
```python
from category_encoders import MEstimateEncoder

# IMPORTANT: Must use pre-fitted encoders from training!
# Don't fit on test data!

# Load pre-fitted encoders
import joblib
store_encoder = joblib.load('store_encoder.pkl')
sku_encoder = joblib.load('sku_encoder.pkl')

# Transform
test['store_encoded'] = store_encoder.transform(test['store_id'])
test['sku_encoded'] = sku_encoder.transform(test['sku_id'])
```

**C. DateTime Features**
```python
from datetime import datetime, timedelta

# Convert to datetime
test['week'] = pd.to_datetime(test['week'], format='%d/%m/%y')
test['weekend_date'] = test['week'] + timedelta(days=6)

# Extract features
start_date = datetime(2011, 1, 17)

test['year'] = test['week'].dt.year
test['date'] = test['week'].dt.day
test['month'] = test['week'].dt.month
test['weekday'] = test['week'].dt.dayofweek
test['weeknum'] = test['week'].dt.isocalendar().week
test['week_serial'] = (test['week'] - start_date).dt.total_seconds() / (86400 * 7)

test['end_year'] = test['weekend_date'].dt.year
test['end_date'] = test['weekend_date'].dt.day
test['end_month'] = test['weekend_date'].dt.month
test['end_weekday'] = test['weekend_date'].dt.dayofweek
test['end_weeknum'] = test['weekend_date'].dt.isocalendar().week
test['end_week_serial'] = (test['weekend_date'] - start_date).dt.total_seconds() / (86400 * 7)

# Apply M-Estimate encoding to time features (use pre-fitted encoder)
time_encoder = joblib.load('time_encoder.pkl')
time_features = ['date', 'end_week_serial', 'month', 'week_serial', 'year', 
                'weekday', 'weeknum', 'end_weekday', 'end_month', 
                'end_weeknum', 'end_date', 'end_year']
test[time_features] = time_encoder.transform(test[time_features])
```

#### Step 5: Prepare Features

```python
# Feature columns used by model
cols_to_use = [
    'base_price', 'total_price', 'diff', 'relative_diff_base', 'relative_diff_total',
    'is_featured_sku', 'is_display_sku', 'store_encoded', 'sku_encoded',
    'store_id', 'sku_id',
    'date', 'end_week_serial', 'month', 'week_serial', 'year', 'weekday', 
    'weeknum', 'end_weekday', 'end_month', 'end_weeknum', 'end_date', 'end_year'
]

X_test = test[cols_to_use]
```

#### Step 6: Load Models & Predict

```python
import lightgbm as lgb
import numpy as np

# Load all 10 fold models
models = []
for fold in range(10):
    model = lgb.Booster(model_file=f'model_fold_{fold}.txt')
    models.append(model)

# Make predictions with each model
predictions_all = []
for model in models:
    pred = model.predict(X_test, num_iteration=model.best_iteration)
    # Inverse log transform
    pred = np.exp(pred)
    predictions_all.append(pred)

# Ensemble: average predictions
final_predictions = np.mean(predictions_all, axis=0)

# Ensure positive values
final_predictions = np.abs(final_predictions)
```

#### Step 7: Format Output

```python
# Create submission
submission = pd.DataFrame({
    'record_ID': test['record_ID'],
    'units_sold': final_predictions
})

# Save
submission.to_csv('submission.csv', index=False)
print(f"‚úì Saved {len(submission)} predictions to submission.csv")
```

### 5.3 Complete Inference Script

```python
def inference_pipeline(test_path, output_path='submission.csv'):
    """
    Complete inference pipeline
    
    Args:
        test_path: Path to test CSV file
        output_path: Path to save predictions
        
    Returns:
        DataFrame with predictions
    """
    import pandas as pd
    import numpy as np
    import lightgbm as lgb
    from datetime import datetime, timedelta
    import joblib
    
    # 1. Load data
    test = pd.read_csv(test_path)
    print(f"Loaded {len(test)} test samples")
    
    # 2. Preprocessing
    test['total_price'] = test['total_price'].fillna(test['base_price'])
    
    # 3. Feature Engineering
    # Price features
    test['diff'] = test['base_price'] - test['total_price']
    test['relative_diff_base'] = test['diff'] / test['base_price']
    test['relative_diff_total'] = test['diff'] / test['total_price']
    
    # Categorical encoding
    store_encoder = joblib.load('store_encoder.pkl')
    sku_encoder = joblib.load('sku_encoder.pkl')
    test['store_encoded'] = store_encoder.transform(test['store_id'])
    test['sku_encoded'] = sku_encoder.transform(test['sku_id'])
    
    # DateTime features
    test['week'] = pd.to_datetime(test['week'], format='%d/%m/%y')
    test['weekend_date'] = test['week'] + timedelta(days=6)
    start_date = datetime(2011, 1, 17)
    
    test['year'] = test['week'].dt.year
    test['date'] = test['week'].dt.day
    test['month'] = test['week'].dt.month
    test['weekday'] = test['week'].dt.dayofweek
    test['weeknum'] = test['week'].dt.isocalendar().week
    test['week_serial'] = (test['week'] - start_date).dt.total_seconds() / (86400 * 7)
    
    test['end_year'] = test['weekend_date'].dt.year
    test['end_date'] = test['weekend_date'].dt.day
    test['end_month'] = test['weekend_date'].dt.month
    test['end_weekday'] = test['weekend_date'].dt.dayofweek
    test['end_weeknum'] = test['weekend_date'].dt.isocalendar().week
    test['end_week_serial'] = (test['weekend_date'] - start_date).dt.total_seconds() / (86400 * 7)
    
    time_encoder = joblib.load('time_encoder.pkl')
    time_features = ['date', 'end_week_serial', 'month', 'week_serial', 'year', 
                    'weekday', 'weeknum', 'end_weekday', 'end_month', 
                    'end_weeknum', 'end_date', 'end_year']
    test[time_features] = time_encoder.transform(test[time_features])
    
    # 4. Prepare features
    cols_to_use = [
        'base_price', 'total_price', 'diff', 'relative_diff_base', 'relative_diff_total',
        'is_featured_sku', 'is_display_sku', 'store_encoded', 'sku_encoded',
        'store_id', 'sku_id',
        'date', 'end_week_serial', 'month', 'week_serial', 'year', 'weekday', 
        'weeknum', 'end_weekday', 'end_month', 'end_weeknum', 'end_date', 'end_year'
    ]
    X_test = test[cols_to_use]
    
    # 5. Load models and predict
    predictions_all = []
    for fold in range(10):
        model = lgb.Booster(model_file=f'model_fold_{fold}.txt')
        pred = model.predict(X_test, num_iteration=model.best_iteration)
        pred = np.exp(pred)  # Inverse transform
        predictions_all.append(pred)
    
    # 6. Ensemble
    final_predictions = np.mean(predictions_all, axis=0)
    final_predictions = np.abs(final_predictions)
    
    # 7. Format output
    submission = pd.DataFrame({
        'record_ID': test['record_ID'],
        'units_sold': final_predictions
    })
    
    # 8. Save
    submission.to_csv(output_path, index=False)
    print(f"‚úì Saved predictions to {output_path}")
    
    return submission

# Usage
predictions = inference_pipeline('test.csv', 'submission.csv')
```

---

## 6. Input Specifications

### 6.1 File Format

**Format:** CSV (Comma Separated Values)  
**Encoding:** UTF-8  
**Line Ending:** LF (\n) ho·∫∑c CRLF (\r\n)  
**Header:** Required (first row)

### 6.2 Column Specifications

| Column | Type | Nullable | Min | Max | Format | Notes |
|--------|------|----------|-----|-----|--------|-------|
| record_ID | int64 | No | 1 | 999999 | Integer | Unique identifier |
| week | object/string | No | - | - | DD/MM/YY | Ng√†y b·∫Øt ƒë·∫ßu tu·∫ßn |
| store_id | int64 | No | 8023 | 9984 | Integer | 76 unique stores |
| sku_id | int64 | No | 216233 | 679023 | Integer | 28 unique SKUs |
| total_price | float64 | Yes | 41.33 | 562.16 | Float | Gi√° b√°n |
| base_price | float64 | No | 61.28 | 562.16 | Float | Gi√° g·ªëc |
| is_featured_sku | int64 | No | 0 | 1 | Binary | Feature flag |
| is_display_sku | int64 | No | 0 | 1 | Binary | Display flag |

### 6.3 Data Constraints

**Business Rules:**
1. `base_price >= total_price` (usually, c√≥ th·ªÉ c√≥ exceptions)
2. `total_price > 0` v√† `base_price > 0`
3. M·ªói (store_id, sku_id, week) combination l√† unique
4. `week` ph·∫£i theo format DD/MM/YY (v√≠ d·ª•: 16/07/13)

**Technical Constraints:**
1. Maximum file size: 100 MB
2. Maximum rows: 1,000,000
3. Character encoding: UTF-8
4. No special characters trong column names

### 6.4 Missing Value Handling

| Column | Missing Value Strategy |
|--------|----------------------|
| total_price | Fill v·ªõi base_price |
| Others | **NOT ALLOWED** - s·∫Ω raise error |

### 6.5 Example Valid Input

```csv
record_ID,week,store_id,sku_id,total_price,base_price,is_featured_sku,is_display_sku
212645,16/07/13,8091,216418,108.30,108.30,0,0
212646,16/07/13,8091,216419,109.01,109.01,0,0
212647,16/07/13,8091,216425,120.50,133.95,1,1
212648,16/07/13,8091,216233,,133.95,0,0
```

**Note:** Row 212648 c√≥ missing total_price ‚Üí s·∫Ω ƒë∆∞·ª£c fill = base_price = 133.95

---

## 7. Output Specifications

### 7.1 File Format

**Format:** CSV  
**Encoding:** UTF-8  
**Columns:** 2 columns (record_ID, units_sold)  
**Rows:** Same as input (13,860 for test set)

### 7.2 Output Schema

| Column | Type | Range | Precision | Description |
|--------|------|-------|-----------|-------------|
| record_ID | int64 | - | - | Copy t·ª´ input, unique identifier |
| units_sold | float64 | [0, ‚àû) | 6 decimals | Predicted s·ªë l∆∞·ª£ng b√°n |

### 7.3 Output Statistics (Expected)

```
units_sold:
    count: 13,860
    mean:  ~45-50
    std:   ~35-40
    min:   ~5-10
    25%:   ~20-25
    50%:   ~35-40
    75%:   ~55-60
    max:   ~200-300
```

### 7.4 Sample Output

```csv
record_ID,units_sold
212645,21.163058
212646,23.675132
212647,34.839319
212648,31.773779
212649,24.315106
```

### 7.5 Post-Processing Rules

1. **Inverse Log Transform:** `units_sold = exp(prediction) - 1`
2. **Absolute Value:** `units_sold = abs(units_sold)` (ensure non-negative)
3. **No Rounding:** Keep as float64 v·ªõi 6 decimals
4. **No Clipping:** Kh√¥ng clip min/max values

---

## 8. Code Examples

### 8.1 Quick Inference (Minimal Code)

```python
# Assuming trained model saved
import pandas as pd
import lightgbm as lgb
import numpy as np

# Load
test = pd.read_csv('test.csv')
model = lgb.Booster(model_file='model.txt')

# Simple prediction (assume features already prepared)
X_test = test[feature_columns]
predictions = np.exp(model.predict(X_test))

# Save
pd.DataFrame({
    'record_ID': test['record_ID'],
    'units_sold': predictions
}).to_csv('submission.csv', index=False)
```

### 8.2 Batch Inference (Large Dataset)

```python
def batch_inference(test_path, batch_size=1000):
    """Process large dataset in batches"""
    import pandas as pd
    import lightgbm as lgb
    import numpy as np
    
    # Load model once
    model = lgb.Booster(model_file='model.txt')
    
    # Process in chunks
    results = []
    for chunk in pd.read_csv(test_path, chunksize=batch_size):
        # Preprocess chunk
        chunk = preprocess_features(chunk)
        
        # Predict
        X_chunk = chunk[feature_columns]
        pred = np.exp(model.predict(X_chunk))
        
        # Store
        results.append(pd.DataFrame({
            'record_ID': chunk['record_ID'],
            'units_sold': pred
        }))
    
    # Combine
    final = pd.concat(results, ignore_index=True)
    return final
```

### 8.3 Real-time Inference (Single Sample)

```python
def predict_single_sample(sample_dict):
    """
    Predict for single sample
    
    Args:
        sample_dict: Dictionary with required fields
        
    Returns:
        Predicted units_sold
        
    Example:
        sample = {
            'week': '16/07/13',
            'store_id': 8091,
            'sku_id': 216418,
            'total_price': 108.30,
            'base_price': 108.30,
            'is_featured_sku': 0,
            'is_display_sku': 0
        }
        units = predict_single_sample(sample)
    """
    import pandas as pd
    import lightgbm as lgb
    import numpy as np
    
    # Convert to DataFrame
    df = pd.DataFrame([sample_dict])
    
    # Feature engineering
    df = preprocess_features(df)
    
    # Predict
    X = df[feature_columns]
    model = lgb.Booster(model_file='model.txt')
    prediction = np.exp(model.predict(X))[0]
    
    return float(prediction)
```

### 8.4 API Endpoint (FastAPI)

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import lightgbm as lgb
import numpy as np

app = FastAPI()

# Load model at startup
model = lgb.Booster(model_file='model.txt')

class PredictionRequest(BaseModel):
    week: str
    store_id: int
    sku_id: int
    total_price: float
    base_price: float
    is_featured_sku: int
    is_display_sku: int

class PredictionResponse(BaseModel):
    record_ID: int
    units_sold: float

@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    try:
        # Feature engineering
        features = engineer_features(request.dict())
        
        # Predict
        prediction = np.exp(model.predict([features]))[0]
        
        return PredictionResponse(
            record_ID=0,  # Generate or pass from request
            units_sold=float(prediction)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## 9. Performance Metrics

### 9.1 Evaluation Metric

**Primary Metric:** RMSLE (Root Mean Squared Logarithmic Error)

**Formula:**
```
RMSLE = sqrt(mean((log(predicted + 1) - log(actual + 1))^2)) * 1000
```

**Why RMSLE?**
- Handle skewed distribution
- Penalize under-prediction v√† over-prediction equally (in log space)
- Less sensitive to outliers
- Business-friendly: focus on relative errors

**Python Implementation:**
```python
def RMSLE(actual, predicted):
    predicted = np.array([np.log(np.abs(x+1.0)) for x in predicted])
    actual = np.array([np.log(np.abs(x+1.0)) for x in actual])
    log_err = actual - predicted
    return 1000 * np.sqrt(np.mean(log_err**2))
```

### 9.2 Model Performance

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Validation RMSLE** | 327.76 | 10-fold CV average |
| **Std Dev (across folds)** | ¬±3.5 | Consistent performance |
| **Min RMSLE** | 323.02 | Best fold |
| **Max RMSLE** | 330.70 | Worst fold |
| **Public LB** | 360.71 | Test set performance |

### 9.3 Inference Speed

**Hardware:** 
- CPU: Intel i7 / AMD Ryzen 7
- RAM: 8GB
- No GPU required

**Benchmarks:**
- Single sample: <1ms
- 1,000 samples: ~50ms
- 13,860 samples: ~0.7s
- Feature engineering: ~2-3s
- Total pipeline: ~4-5s

### 9.4 Model Size

```
Total size: ~15 MB (10 models)
‚îú‚îÄ‚îÄ model_fold_0.txt: ~1.5 MB
‚îú‚îÄ‚îÄ model_fold_1.txt: ~1.5 MB
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ model_fold_9.txt: ~1.5 MB
‚îú‚îÄ‚îÄ store_encoder.pkl: ~5 KB
‚îú‚îÄ‚îÄ sku_encoder.pkl: ~3 KB
‚îî‚îÄ‚îÄ time_encoder.pkl: ~2 KB
```

---

## 10. Troubleshooting

### 10.1 Common Errors

#### Error 1: KeyError - Missing Column

**Error Message:**
```
KeyError: 'column_name'
```

**Cause:** Input CSV thi·∫øu required column

**Solution:**
```python
# Verify columns
required = ['record_ID', 'week', 'store_id', 'sku_id', 
            'total_price', 'base_price', 'is_featured_sku', 'is_display_sku']
missing = set(required) - set(df.columns)
if missing:
    print(f"Missing columns: {missing}")
```

#### Error 2: DateParseError

**Error Message:**
```
ValueError: time data '2013-07-16' does not match format '%d/%m/%y'
```

**Cause:** Week column c√≥ wrong format

**Solution:**
```python
# Check and convert format
df['week'] = pd.to_datetime(df['week'], format='%d/%m/%y')
# or auto-detect
df['week'] = pd.to_datetime(df['week'], infer_datetime_format=True)
```

#### Error 3: Categorical Value Not Seen in Training

**Error Message:**
```
KeyError: store_id 9999 not found in encoder
```

**Cause:** Test set c√≥ store/SKU kh√¥ng c√≥ trong training

**Solution:**
```python
# Use handle_unknown parameter
encoder = MEstimateEncoder(handle_unknown='value', handle_missing='value')
```

#### Error 4: Negative Predictions

**Cause:** M·ªôt s·ªë predictions c√≥ th·ªÉ √¢m sau inverse transform

**Solution:**
```python
# Apply absolute value
predictions = np.abs(predictions)
```

### 10.2 Data Quality Checks

```python
def validate_input(df):
    """Comprehensive input validation"""
    errors = []
    
    # Check columns
    required_cols = ['record_ID', 'week', 'store_id', 'sku_id', 
                     'total_price', 'base_price', 'is_featured_sku', 'is_display_sku']
    missing = set(required_cols) - set(df.columns)
    if missing:
        errors.append(f"Missing columns: {missing}")
    
    # Check data types
    if df['store_id'].dtype not in ['int64', 'int32']:
        errors.append("store_id must be integer")
    
    # Check value ranges
    if (df['is_featured_sku'].notna() & ~df['is_featured_sku'].isin([0, 1])).any():
        errors.append("is_featured_sku must be 0 or 1")
    
    # Check duplicates
    if df['record_ID'].duplicated().any():
        errors.append("Duplicate record_IDs found")
    
    # Check missing
    critical_missing = df[['week', 'store_id', 'sku_id', 'base_price']].isnull().sum()
    if critical_missing.any():
        errors.append(f"Missing critical values: {critical_missing[critical_missing > 0]}")
    
    return errors

# Usage
errors = validate_input(test_df)
if errors:
    for err in errors:
        print(f"‚ùå {err}")
else:
    print("‚úì Validation passed!")
```

### 10.3 Performance Issues

**Problem:** Inference qu√° ch·∫≠m

**Solutions:**
1. **Use fewer models:** D√πng 3-5 models thay v√¨ 10
2. **Batch processing:** Process nhi·ªÅu samples c√πng l√∫c
3. **Cache encoders:** Load encoders m·ªôt l·∫ßn, reuse nhi·ªÅu l·∫ßn
4. **Optimize feature engineering:** Vectorize operations

```python
# Slow
for i, row in df.iterrows():
    df.loc[i, 'diff'] = row['base_price'] - row['total_price']

# Fast (vectorized)
df['diff'] = df['base_price'] - df['total_price']
```

### 10.4 Memory Issues

**Problem:** Out of memory v·ªõi large dataset

**Solutions:**
1. **Chunk processing:** Read CSV in chunks
2. **Reduce dtypes:** Use float32 instead of float64
3. **Delete unused columns:** Drop columns after use

```python
# Memory optimization
df = df.astype({
    'base_price': 'float32',
    'total_price': 'float32',
    'store_id': 'int16',
    'sku_id': 'int32'
})
```

---

## 11. Appendix

### 11.1 Glossary

| Term | Definition |
|------|------------|
| **SKU** | Stock Keeping Unit - unique identifier cho s·∫£n ph·∫©m |
| **Store** | C·ª≠a h√†ng / ƒëi·ªÉm b√°n |
| **Week** | Tu·∫ßn b·∫Øt ƒë·∫ßu t·ª´ ng√†y ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh |
| **Units Sold** | S·ªë l∆∞·ª£ng s·∫£n ph·∫©m ƒë√£ b√°n |
| **Base Price** | Gi√° g·ªëc c·ªßa s·∫£n ph·∫©m |
| **Total Price** | Gi√° b√°n th·ª±c t·∫ø (sau discount) |
| **Featured SKU** | S·∫£n ph·∫©m ƒë∆∞·ª£c highlight/qu·∫£ng c√°o |
| **Display SKU** | S·∫£n ph·∫©m ƒë∆∞·ª£c display ·ªü v·ªã tr√≠ ƒë·∫∑c bi·ªát |
| **RMSLE** | Root Mean Squared Logarithmic Error - metric ƒë√°nh gi√° |
| **DART** | Dropouts meet Multiple Additive Regression Trees - LightGBM boosting type |
| **M-Estimate** | Target encoding technique v·ªõi regularization |
| **Ensemble** | K·∫øt h·ª£p nhi·ªÅu models ƒë·ªÉ improve prediction |

### 11.2 Reference Links

- **LightGBM Documentation:** https://lightgbm.readthedocs.io/
- **Category Encoders:** http://contrib.scikit-learn.org/category_encoders/
- **Pandas Documentation:** https://pandas.pydata.org/docs/
- **RMSLE Metric:** https://www.kaggle.com/c/demand-forecasting-kernels-only/overview/evaluation

### 11.3 Model Artifacts

**Required Files:**
```
project/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ model_fold_0.txt
‚îÇ   ‚îú‚îÄ‚îÄ model_fold_1.txt
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ model_fold_9.txt
‚îú‚îÄ‚îÄ encoders/
‚îÇ   ‚îú‚îÄ‚îÄ store_encoder.pkl
‚îÇ   ‚îú‚îÄ‚îÄ sku_encoder.pkl
‚îÇ   ‚îî‚îÄ‚îÄ time_encoder.pkl
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ feature_columns.json
‚îî‚îÄ‚îÄ inference.py
```

### 11.4 Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | Nov 2024 | Initial release v·ªõi 10-fold CV |
| 1.1 | - | Planned: Hyperparameter tuning |
| 1.2 | - | Planned: Feature selection |

### 11.5 Contact & Support
---

## üìù Summary Checklist

Tr∆∞·ªõc khi ch·∫°y inference, ƒë·∫£m b·∫£o:

- [ ] Python 3.7+ installed
- [ ] All required libraries installed (`pip install -r requirements.txt`)
- [ ] Test data format correct (CSV v·ªõi 8 columns)
- [ ] All model files available (10 fold models + 3 encoders)
- [ ] Week column theo format DD/MM/YY
- [ ] No missing values trong critical columns
- [ ] Output directory c√≥ write permission

**Expected Runtime:** 4-5 gi√¢y cho 13,860 samples

**Expected Output:** CSV file v·ªõi 2 columns (record_ID, units_sold)

---

**END OF DOCUMENT**

*Last updated: November 2025*

